{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION\n",
    "\n",
    "<hr>\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/45539698/53720323-791d1680-3e86-11e9-9ceb-03d640db8131.png)\n",
    "![image](https://user-images.githubusercontent.com/45539698/53715306-66e7ac00-3e77-11e9-896e-b7b808da3614.png)\n",
    "![image](https://user-images.githubusercontent.com/45539698/53715309-694a0600-3e77-11e9-82f1-225fa7055d29.png)\n",
    "![image](https://user-images.githubusercontent.com/45539698/53715314-6c44f680-3e77-11e9-8a10-bc586709e171.png)\n",
    "\n",
    "#### To understand odds and log of odds, let’s take this example:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/45539698/53715323-7961e580-3e77-11e9-868d-fd7fef96ee82.png)\n",
    "\n",
    "Example: \n",
    "Team winning =1. Team Loosing=6. Odds = 1/6 = 0.16\n",
    "Team winning=6. Team Loosing=1.\n",
    "On a Standard Number Line, the ratio 1/6 and 6/1 is plotted as :\n",
    "![image](https://user-images.githubusercontent.com/45539698/53715335-8088f380-3e77-11e9-9856-51f389f7028d.png)\n",
    "\n",
    "#### But if we take logs, then we can prepare symmetry. That is:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/45539698/53715352-8979c500-3e77-11e9-872c-ca1afdb11c2f.png)\n",
    "![image](https://user-images.githubusercontent.com/45539698/53715359-8d0d4c00-3e77-11e9-92ab-459208a249e6.png)\n",
    "![image](https://user-images.githubusercontent.com/45539698/53715367-93032d00-3e77-11e9-9beb-830f59a0888b.png)\n",
    "![image](https://user-images.githubusercontent.com/45539698/53715370-95fe1d80-3e77-11e9-8c88-4d672b1a6076.png)\n",
    "![image](https://user-images.githubusercontent.com/45539698/53715373-98607780-3e77-11e9-9450-1046a91d041b.png)\n",
    "![image](https://user-images.githubusercontent.com/45539698/53715376-9bf3fe80-3e77-11e9-8426-58ef1ccd8b63.png)\n",
    "![image](https://user-images.githubusercontent.com/45539698/53715382-a2827600-3e77-11e9-9978-67404cb459f1.png)\n",
    "\n",
    "#### Therefore we will use the concept of MAXIMUM LIKELIHOOD.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/45539698/53715386-ac0bde00-3e77-11e9-8a21-777ee9d22c03.png)\n",
    "![image](https://user-images.githubusercontent.com/45539698/53715389-af06ce80-3e77-11e9-8927-2ace430b512e.png)\n",
    "![image](https://user-images.githubusercontent.com/45539698/53715393-b201bf00-3e77-11e9-9c36-c583ec08cc41.png)\n",
    "\n",
    "#### As we saw that we will use the concept of MAXIMUM LIKELIHOOD. The difference between Likelihood and Probability is:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/45539698/53715410-bb8b2700-3e77-11e9-8cc3-e92a1bbc1195.png)\n",
    "\n",
    "#### But, interesting thing to note in Logistic Regression is, Probability and Likelihood means the same:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/45539698/53715426-c3e36200-3e77-11e9-98f2-8ced96ffe0c6.png)\n",
    "\n",
    "#### Now we will calculate likelihood score for obesity and non obesity. \n",
    "\n",
    "![image](https://user-images.githubusercontent.com/45539698/53715433-ca71d980-3e77-11e9-9d35-7a180438849c.png)\n",
    "\n",
    "#### After computing the value= -3.77\n",
    "#### Now, we rotate the line and make projection.  Plot the squiggle from the projection.  And, then again compute the likelihood score:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/45539698/53715442-d2ca1480-3e77-11e9-9307-ec5d1f4a1b94.png)\n",
    "![image](https://user-images.githubusercontent.com/45539698/53715446-d8275f00-3e77-11e9-80f3-e8d76e8bcbb2.png)\n",
    "\n",
    "#### This time the score came out to be -4.15. \n",
    "#### Keep repeating the same. Till we get the maximum likelihood score. When reached that score that means that projection is best, that squiggle is best \n",
    "\n",
    "#### Now once we decide the shape of the curve, next comes the selection of Threshold Point. This is done via computing ROC-AUC. \n",
    "Lets assume we are classifying rats as **obese [1]** and **non obese[0]** on the basis of their weights.For classification purpose we are choosing **Logistic regression**.\n",
    "\n",
    " - The **RED** curve in the below diagram represents **NON-OBESITY**.\n",
    " - **BLUE** represents **OBESITY** \n",
    " - **NON-OBESITY** means weights less than theshold (0.5)\n",
    " - **OBESITY** means weights above the threshold.\n",
    " \n",
    "![image](https://user-images.githubusercontent.com/45539698/52516971-98df6700-2c59-11e9-8ea4-fc8d734c6943.png)\n",
    "\n",
    "If we decrease the threshold (< 0.5) the **RED** part of the curve will minimise, and **BLUE** part increase. If we increase  the threshold (> 0.5) the **RED** part of the curve will increase, and **BLUE** part decrease.\n",
    "\n",
    "#### Hence, for each threshod value, the classification will be impacted at each step. Some of those which are non-obese will be classified as obese and  some of those which are obese will be classified as non-obese. Therefore, for different  thresholds, there will be different  **Confusion Matricies**\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/45539698/52517037-bc56e180-2c5a-11e9-8d65-f34bb7960511.png)\n",
    "\n",
    "\n",
    "As we can see - For different thresholds (T1, T2, T3), there will be different Confusion Matricies. Example for **T5**:\n",
    "![image](https://user-images.githubusercontent.com/45539698/52518688-8fafc380-2c74-11e9-8055-04f63da0e71d.png)\n",
    "\n",
    "Similarly, for each threshold, there will be numerous Confusion Matricies.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/45539698/52518233-80794780-2c6d-11e9-8fe9-4e11a0f424d9.png)\n",
    "\n",
    "#### Now, the question arises that which threshold should be chosen ? To make this clear we plot ROC Curves. The fundamental of ROC cuves depend upon the two important Formulas - TPR(True Positive Rate) and FPR(False Positive Rate), because ROC curves are plotted between TPR vs. FPR. \n",
    "\n",
    "![image](https://user-images.githubusercontent.com/45539698/52517258-d5fa2800-2c5e-11e9-82dc-2990c9f9f0ef.png)\n",
    "\n",
    "\n",
    "As we have seen this confusion Matrix earlier, let's now see how we calculate TPR and FPR and how we plot it on ROC Curve:\n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/45539698/52518688-8fafc380-2c74-11e9-8055-04f63da0e71d.png)\n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/45539698/52518294-7f94e580-2c6e-11e9-8e0e-36615fbd9c0f.png)\n",
    "\n",
    "\n",
    "After the ploting is done, we get a ROC curve as below:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/45539698/52518753-b6222e80-2c75-11e9-852b-ab9d58024b2b.png)\n",
    "\n",
    "\n",
    "On first glance we can say that:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/45539698/52518774-23ce5a80-2c76-11e9-9c90-f13f772a9af6.png)\n",
    "\n",
    "\n",
    "Now we can plot various ROC Curves (like above) for different Classification Algorithms, and then compare the Area Under The Curve to choose which algorithm to pick. the hight the AUC, the better the algorithm.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/45539698/52518824-36955f00-2c77-11e9-9f46-c41c76b99500.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the complete File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisCompleteData= pd.read_csv('C://Users//Lenovo//Desktop//IPY//KNN//iris.csv')\n",
    "irisCompleteData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = irisCompleteData.head(140)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The reamining 8 records will be uconsidered as untouched data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 5)\n",
      "(8, 4)\n"
     ]
    }
   ],
   "source": [
    "untouchedTestData = irisCompleteData.tail(8)\n",
    "print(untouchedTestData.shape)\n",
    "del untouchedTestData['class']\n",
    "print(untouchedTestData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9642857142857143\n"
     ]
    }
   ],
   "source": [
    "selectdColumns = [\"sepallength\",\"sepalwidth\",\"petallength\",\"petalwidth\"]\n",
    "\n",
    "all_X = data[selectdColumns]\n",
    "all_y = data['class']\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    all_X, all_y, random_state=0, test_size=0.20)\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(train_X, train_y)\n",
    "\n",
    "predictions = logistic.predict(test_X)\n",
    "accuracy = accuracy_score(test_y, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_observation = [[5.6,2.7,4.0,1.2]]\n",
    "# Predict class\n",
    "logistic.predict(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores = cross_val_score(logistic, all_X, all_y, cv=10)\n",
    "#scores.sort()\n",
    "#accuracy = scores.mean()\n",
    "\n",
    "#print(scores)\n",
    "#print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
