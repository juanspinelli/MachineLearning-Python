{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert sentence into Lemma -----> Removal of Pronoun -----> Removal of  Stop Words ----> Removal of Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "docx = nlp(\"This is how John Walker was walking. He was also running beside the lawn.\")\n",
    "dataAfterLemmaFilter = []\n",
    "dataAfterPronounFilter = []\n",
    "dataAfterStopwords =[]\n",
    "dataAfterPunctuations =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in docx:\n",
    "    dataAfterLemmaFilter.append(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in dataAfterLemmaFilter:\n",
    "    if token != \"-PRON-\":\n",
    "        dataAfterPronounFilter.append(token.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(STOP_WORDS)\n",
    "\n",
    "for token in dataAfterPronounFilter:\n",
    "    if token != stopwords:\n",
    "        dataAfterStopwords.append(token) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = string.punctuation\n",
    "\n",
    "for token in dataAfterStopwords:\n",
    "    if token not in punctuations:\n",
    "        dataAfterPunctuations.append(token) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix from Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love Brazil, but I think Sweden is best.', 'Not to mention that Germany beats both.']\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "  (0, 1)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 7)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 9)\t1\n",
      "['beats', 'best', 'both', 'brazil', 'but', 'germany', 'is', 'love', 'mention', 'not', 'sweden', 'that', 'think', 'to']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beats</th>\n",
       "      <th>best</th>\n",
       "      <th>both</th>\n",
       "      <th>brazil</th>\n",
       "      <th>but</th>\n",
       "      <th>germany</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>mention</th>\n",
       "      <th>not</th>\n",
       "      <th>sweden</th>\n",
       "      <th>that</th>\n",
       "      <th>think</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   beats  best  both  brazil  but  germany  is  love  mention  not  sweden  \\\n",
       "0      0     1     0       1    1        0   1     1        0    0       1   \n",
       "1      1     0     1       0    0        1   0     0        1    1       0   \n",
       "\n",
       "   that  think  to  \n",
       "0     0      1   0  \n",
       "1     1      0   1  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I love Brazil, but I think Sweden is best. Not to mention that Germany beats both.\"\n",
    "doc = nlp(sentence)\n",
    "text_data=[sent.string.strip() for sent in doc.sents]\n",
    "print(text_data)\n",
    "\n",
    "count = CountVectorizer()\n",
    "print(count)\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "print(bag_of_words)\n",
    "feature_names = count.get_feature_names()\n",
    "print(feature_names)\n",
    "pd.DataFrame(bag_of_words.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
